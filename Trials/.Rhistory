test_set$pred <- factor(ifelse(test_set$Class1 >= .5, "Class1", "Class2"))
head(test_set)
confusionMatrix(data = test_set$pred, reference = test_set$obs)
confusionMatrix(data = test_set$pred, reference = test_set$obs, mode = "prec_recall")
twoClassSummary(test_set, lev = levels(test_set$obs))
prSummary(test_set, lev = levels(test_set$obs))
head(test_set)
all_datasets <- readRDS("I:/.shortcut-targets-by-id/1J80DpjKDrh53PVd45dYrJmKvpEWC0iiz/subsampling/Data/results/all_datasets.rds")
trainData <- all_datasets$data[all_datasets$dataset=="new-thyroid1"][[1]]
trainData %<>% mutate_if(is.character, as.factor) %>%
clean_names() %>%
mutate(class = relevel(class, ref = 'positive'))
recSteps = recipe(class ~ ., data = trainData) %>%
step_nzv( all_predictors() ) %>%
step_BoxCox(all_numeric()) %>%
step_normalize( all_numeric() ) %>%
step_dummy(all_nominal(), -all_outcomes() ) # create dummy variables for categorical predictors
outputFun = function(data, lev=levels(data$class), model = NULL) {
defaultS = defaultSummary(data, lev, model) # returns Accuracy and Kappa
twoClassS = twoClassSummary(data, lev, model) # returns AUC, sensitivity and specificity
prS = prSummary(data, lev, model)
out = c(defaultS, twoClassS, prS)
out['F'] = ifelse(!is.finite(out['F']), NA, out['F'])
out['AUC'] = ifelse(!is.finite(out['AUC']), NA, out['AUC'])
return(out)
}
fitControl = trainControl(
method = "cv", # k-fold cross validation
number = 10, # Number of Folds
search = "grid", # Default grid search for parameter tuning
sampling = "down", # If none, please set to NULL
summaryFunction = outputFun, # see custom function in this file
classProbs = T, # should class probabilities be returned
selectionFunction = "best", # best fold
savePredictions = 'final',
index = createResample(trainData$class, 10))
im_fit <- train(class~.,, data = trainData, method = "glm", metric = "AUC",
trControl = fitControl)
result <- evalm(list(im_fit), positive="positive")
head(result$probs)
im_fit$results
result$optres
result$roc
head(im_fit$pred)
twoClassSummary(im_fit$pred, lev=levels(trainData$class))
prSummary(im_fit$pred, lev=levels(trainData$class))
table(im_fit$pred$obs, im_fit$pred)
table(im_fit$pred$obs, im_fit$pred$pred)
levels(trainData$class)
temp <- im_fit$pred %>% select(obs, positive, negative, pred)
twoClassSummary(temp)
head(temp)
twoClassSummary(temp, lev=levels(temp$obs))
prSummary(temp, lev=levels(temp$obs))
table(temp$obs, temp$pred)
summary(temp$positive[temp$obs=="positive"])
summary(temp$positive[temp$obs=="negative"])
?twoClassSummary
im_fit$results
result$roc
result$optres
twoClassSummary(temp, lev=levels(temp$obs))
prSummary(temp, lev=levels(temp$obs))
table(temp$obs, temp$pred)
?confusionMatrix
table(temp$pred, temp$obs)
fg <- im_fit$pred$positive[im_fit$pred$obs=="positive"]
bg <- im_fit$pred$positive[im_fit$pred$obs=="negative"]
# ROC Curve
roc <- roc.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
plot(roc)
# PR Curve
pr <- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
plot(pr)
0.1249390+0.8767064
set.seed(144)
true_class <- factor(sample(paste0("Class", 1:2),
size = 1000,
prob = c(.2, .8), replace = TRUE))
true_class <- sort(true_class)
class1_probs <- rbeta(sum(true_class == "Class1"), 4, 1)
class2_probs <- rbeta(sum(true_class == "Class2"), 1, 2.5)
test_set <- data.frame(obs = true_class,
Class1 = c(class1_probs, class2_probs))
test_set$Class2 <- 1 - test_set$Class1
test_set$pred <- factor(ifelse(test_set$Class1 >= .5, "Class1", "Class2"))
confusionMatrix(data = test_set$pred, reference = test_set$obs)
twoClassSummary(test_set, lev = levels(test_set$obs))
prSummary(test_set, lev = levels(test_set$obs))
head(test_set)
fg <- test_set$Class1[test_set$obs=="Class1"]
bg <- test_set$Class1[test_set$obs=="Class2"]
# ROC Curve
roc <- roc.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
plot(roc)
# PR Curve
pr <- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
plot(pr)
im_fit$results
1-im_fit$re\
1-im_fit$results["AUC"]
fg <- im_fit$pred$positive[im_fit$pred$obs=="positive"]
bg <- im_fit$pred$positive[im_fit$pred$obs=="negative"]
# ROC Curve
roc <- roc.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
# PR Curve
pr <- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
plot(pr)
im_fit$resampledCM
im_fit$terms
all_datasets <- readRDS("I:/.shortcut-targets-by-id/1J80DpjKDrh53PVd45dYrJmKvpEWC0iiz/subsampling/Data/results/all_datasets.rds")
trainData <- all_datasets$data[all_datasets$dataset=="new-thyroid1"][[1]]
trainData %<>% mutate_if(is.character, as.factor) %>%
clean_names() %>%
mutate(class = relevel(class, ref = 'positive'))
recSteps = recipe(class ~ ., data = trainData) %>%
step_nzv( all_predictors() ) %>%
step_BoxCox(all_numeric()) %>%
step_normalize( all_numeric() ) %>%
step_dummy(all_nominal(), -all_outcomes() ) # create dummy variables for categorical predictors
outputFun = function(data, lev=levels(data$class), model = NULL) {
defaultS = defaultSummary(data, lev, model) # returns Accuracy and Kappa
twoClassS = twoClassSummary(data, lev, model) # returns AUC, sensitivity and specificity
prS = prSummary(data, lev, model)
out = c(defaultS, twoClassS, prS)
out['F'] = ifelse(!is.finite(out['F']), NA, out['F'])
out['AUC'] = ifelse(!is.finite(out['AUC']), NA, out['AUC'])
return(out)
}
fitControl = trainControl(
method = "cv", # k-fold cross validation
number = 10, # Number of Folds
search = "grid", # Default grid search for parameter tuning
sampling = "down", # If none, please set to NULL
summaryFunction = outputFun, # see custom function in this file
classProbs = T, # should class probabilities be returned
selectionFunction = "best", # best fold
savePredictions = 'final',
index = createResample(trainData$class, 10))
table(trainData$class)
im_fit <- train(class~.,, data = trainData, method = "glm", metric = "AUC",
trControl = fitControl)
outputFun = function(data, lev=NULL, model = NULL) {
defaultS = defaultSummary(data, lev, model) # returns Accuracy and Kappa
twoClassS = twoClassSummary(data, lev, model) # returns AUC, sensitivity and specificity
prS = prSummary(data, lev, model)
out = c(defaultS, twoClassS, prS)
out['F'] = ifelse(!is.finite(out['F']), NA, out['F'])
out['AUC'] = ifelse(!is.finite(out['AUC']), NA, out['AUC'])
return(out)
}
fitControl = trainControl(
method = "cv", # k-fold cross validation
number = 10, # Number of Folds
search = "grid", # Default grid search for parameter tuning
sampling = "down", # If none, please set to NULL
summaryFunction = outputFun, # see custom function in this file
classProbs = T, # should class probabilities be returned
selectionFunction = "best", # best fold
savePredictions = 'final',
index = createResample(trainData$class, 10))
im_fit <- train(class~.,, data = trainData, method = "glm", metric = "AUC",
trControl = fitControl)
im_fit$results
result <- evalm(list(im_fit))
result$optres
?evalm
result <- evalm(list(im_fit), positive = "positive")
result$optres
fg <- im_fit$pred$positive[im_fit$pred$obs=="positive"]
bg <- im_fit$pred$positive[im_fit$pred$obs=="negative"]
# ROC Curve
roc <- roc.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
plot(roc)
# PR Curve
pr <- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
plot(pr)
temp <- im_fit$pred %>% select(obs, positive, negative, pred)
twoClassSummary(temp)
twoClassSummary(temp, lev=NULL)
levels(temp$obs)
twoClassSummary(temp, lev=levels(temp$obs))
is.factor(temp$obs)
?prSummary
outputFun = function(data, lev=levels(data$class), model = NULL) {
defaultS = defaultSummary(data, lev, model) # returns Accuracy and Kappa
twoClassS = twoClassSummary(data, lev, model) # returns AUC, sensitivity and specificity
prS = prSummary(data, lev, model)
out = c(defaultS, twoClassS, prS)
out['F'] = ifelse(!is.finite(out['F']), NA, out['F'])
out['AUC'] = ifelse(!is.finite(out['AUC']), NA, out['AUC'])
return(out)
}
fitControl = trainControl(
method = "cv", # k-fold cross validation
number = 10, # Number of Folds
search = "grid", # Default grid search for parameter tuning
sampling = "down", # If none, please set to NULL
summaryFunction = outputFun, # see custom function in this file
classProbs = T, # should class probabilities be returned
selectionFunction = "best", # best fold
savePredictions = 'final',
index = createResample(trainData$class, 10))
im_fit <- train(class~.,, data = trainData, method = "glm", metric = "AUC",
trControl = fitControl)
result <- evalm(list(im_fit), positive = "positive")
warnings()
result <- evalm(list(im_fit), positive = "positive")
result$optres
im_fit$results
fg <- im_fit$pred$positive[im_fit$pred$obs=="positive"]
bg <- im_fit$pred$positive[im_fit$pred$obs=="negative"]
# ROC Curve
roc <- roc.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
plot(roc)
# PR Curve
pr <- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
plot(pr)
plot(pr)
all_datasets <- readRDS("I:/.shortcut-targets-by-id/1J80DpjKDrh53PVd45dYrJmKvpEWC0iiz/subsampling/Data/results/all_datasets.rds")
trainData <- all_datasets$data[all_datasets$dataset=="new-thyroid1"][[1]]
trainData %<>% mutate_if(is.character, as.factor) %>%
clean_names() %>%
mutate(class = relevel(class, ref = 'positive'))
recSteps = recipe(class ~ ., data = trainData) %>%
step_nzv( all_predictors() ) %>%
step_BoxCox(all_numeric()) %>%
step_normalize( all_numeric() ) %>%
step_dummy(all_nominal(), -all_outcomes() ) # create dummy variables for categorical predictors
outputFun = function(data, lev=levels(data$class), model = NULL) {
defaultS = defaultSummary(data, lev, model) # returns Accuracy and Kappa
twoClassS = twoClassSummary(data, lev, model) # returns AUC, sensitivity and specificity
prS = prSummary(data, lev, model)
out = c(defaultS, twoClassS, prS)
out['F'] = ifelse(!is.finite(out['F']), NA, out['F'])
out['AUC'] = ifelse(!is.finite(out['AUC']), NA, out['AUC'])
return(out)
}
fitControl = trainControl(
method = "cv", # k-fold cross validation
number = 10, # Number of Folds
search = "grid", # Default grid search for parameter tuning
sampling = "down", # If none, please set to NULL
summaryFunction = outputFun, # see custom function in this file
classProbs = T, # should class probabilities be returned
selectionFunction = "best", # best fold
savePredictions = 'final',
index = createResample(trainData$class, 10))
im_fit <- train(class~.,, data = trainData, method = "glm", metric = "AUC",
trControl = fitControl)
result <- evalm(list(im_fit), positive = "positive")
im_fit$results
result$optres
fg <- im_fit$pred$positive[im_fit$pred$obs=="positive"]
bg <- im_fit$pred$positive[im_fit$pred$obs=="negative"]
# ROC Curve
roc <- roc.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
plot(roc)
# PR Curve
pr <- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
plot(pr)
df <- readRDS("I:/.shortcut-targets-by-id/1J80DpjKDrh53PVd45dYrJmKvpEWC0iiz/subsampling/Trials/all_Result.RDS")
df %<>% filter(measure %in% c("Accuracy", "PR_AUC", "ROC_AUC", "Sens", "Spec"))
df1 <- df %>% filter(dataset=="new-thyroid1")
df1 %<>% filter(measure %in% c("Accuracy", "PR_AUC", "ROC_AUC", "Precision","Sens", "Spec"))
df1 %>% filter(method=="GLM") %>% group_by(metric, subsampling) %>%
ggplot(aes(x = measure, colour = )) +
geom_point(aes(y = value), size = 1.5) +
theme_bw(base_size = 7) + ylim(0, 1) +
facet_grid(rows = vars(metric), cols = vars(subsampling)) +
theme(legend.position = "top") + scale_color_npg()
library(ggsci)
df1 %>% filter(method=="GLM") %>% group_by(metric, subsampling) %>%
ggplot(aes(x = measure, colour = )) +
geom_point(aes(y = value), size = 1.5) +
theme_bw(base_size = 7) + ylim(0, 1) +
facet_grid(rows = vars(metric), cols = vars(subsampling)) +
theme(legend.position = "top") + scale_color_npg()
im_fit <- train(class~.,, data = trainData, method = "glm", metric = "Sens",
trControl = fitControl)
all_datasets <- readRDS("I:/.shortcut-targets-by-id/1J80DpjKDrh53PVd45dYrJmKvpEWC0iiz/subsampling/Data/results/all_datasets.rds")
trainData <- all_datasets$data[all_datasets$dataset=="new-thyroid1"][[1]]
trainData %<>% mutate_if(is.character, as.factor) %>%
clean_names() %>%
mutate(class = relevel(class, ref = 'positive'))
recSteps = recipe(class ~ ., data = trainData) %>%
step_nzv( all_predictors() ) %>%
step_BoxCox(all_numeric()) %>%
step_normalize( all_numeric() ) %>%
step_dummy(all_nominal(), -all_outcomes() ) # create dummy variables for categorical predictors
outputFun = function(data, lev=levels(data$class), model = NULL) {
defaultS = defaultSummary(data, lev, model) # returns Accuracy and Kappa
twoClassS = twoClassSummary(data, lev, model) # returns AUC, sensitivity and specificity
prS = prSummary(data, lev, model)
out = c(defaultS, twoClassS, prS)
out['F'] = ifelse(!is.finite(out['F']), NA, out['F'])
out['AUC'] = ifelse(!is.finite(out['AUC']), NA, out['AUC'])
return(out)
}
fitControl = trainControl(
method = "cv", # k-fold cross validation
number = 10, # Number of Folds
search = "grid", # Default grid search for parameter tuning
sampling = "down", # If none, please set to NULL
summaryFunction = outputFun, # see custom function in this file
classProbs = T, # should class probabilities be returned
selectionFunction = "best", # best fold
savePredictions = 'final',
index = createResample(trainData$class, 10))
im_fit <- train(class~.,, data = trainData, method = "glm", metric = "Sens",
trControl = fitControl)
result <- evalm(list(im_fit), positive = "positive")
im_fit$results
result$optres
?trainControl
set.seed(123)
seeds <- vector(mode = "list", length = 51)
for(i in 1:50) seeds[[i]] <- sample.int(1000, 22)
## For the last model:
seeds[[51]] <- sample.int(1000, 1)
fitControl = trainControl(
method = "cv", # k-fold cross validation
number = 10, # Number of Folds
search = "grid", # Default grid search for parameter tuning
sampling = "down", # If none, please set to NULL
summaryFunction = outputFun, # see custom function in this file
classProbs = T, # should class probabilities be returned
selectionFunction = "best", # best fold
savePredictions = 'final',
index = createResample(trainData$class, 10), seeds = seeds)
im_fit <- train(class~.,, data = trainData, method = "glm", metric = "Sens",
trControl = fitControl)
im_fit$results
im_fit <- train(class~.,, data = trainData, method = "glm", metric = "AUC",
trControl = fitControl)
im_fit$results
im_fit <- train(class~.,, data = trainData, method = "glm", metric = "Accuracy",
trControl = fitControl)
im_fit$results
?train
fitControl = trainControl(
method = "cv", # k-fold cross validation
number = 10, # Number of Folds
search = "grid", # Default grid search for parameter tuning
sampling = NULL, # If none, please set to NULL
summaryFunction = outputFun, # see custom function in this file
classProbs = T, # should class probabilities be returned
selectionFunction = "best", # best fold
savePredictions = 'final',
index = createResample(trainData$class, 10), seeds = seeds)
im_fit <- train(class~.,, data = trainData, method = "glm", metric = "Accuracy",
trControl = fitControl)
im_fit$results
im_fit <- train(class~.,, data = trainData, method = "glm", metric = "Sens",
trControl = fitControl)
im_fit$results
im_fit <- train(class~.,, data = trainData, method = "glm", metric = "AUC",
trControl = fitControl)
im_fit$results
im_fit <- train(class~.,, data = trainData, method = "rpart", metric = "AUC",
trControl = fitControl)
im_fit$results
im_fit <- train(class~.,, data = trainData, method = "rpart", metric = "Accuracy",
trControl = fitControl)
result <- evalm(list(im_fit), positive = "positive")
im_fit$results
im_fit <- train(class~.,, data = trainData, method = "rpart", metric = "Sens",
trControl = fitControl)
im_fit$results
im_fit <- train(class~.,, data = trainData, method = "rpart", metric = "Kappa",
trControl = fitControl)
im_fit$results
im_fit <- train(class~.,, data = trainData, method = "rpart", metric = "tessa",
trControl = fitControl)
im_fit <- caret::train(class~.,, data = trainData, method = "rpart", metric = "Kappa",
trControl = fitControl)
im_fit$results
fitControl = trainControl(
method = "cv", # k-fold cross validation
number = 10, # Number of Folds
search = "grid", # Default grid search for parameter tuning
sampling = "up", # If none, please set to NULL
summaryFunction = outputFun, # see custom function in this file
classProbs = T, # should class probabilities be returned
selectionFunction = "best", # best fold
savePredictions = 'final',
index = createResample(trainData$class, 10), seeds = seeds)
im_fit <- caret::train(class~.,, data = trainData, method = "rpart", metric = "Kappa",
trControl = fitControl)
im_fit$results
im_fit <- caret::train(class~.,, data = trainData, method = "rpart", metric = "AUC",
trControl = fitControl)
im_fit$results
pacman::p_load(tidyverse, magrittr, conflicted)
pacman::p_load(tidyverse, magrittr, janitor,
caret, caretEnsemble, recipes,
rpart, glmnet, conflicted)
?train
?prefer
all_datasets <- readRDS("I:/.shortcut-targets-by-id/1J80DpjKDrh53PVd45dYrJmKvpEWC0iiz/subsampling/Data/results/all_datasets.rds")
trainData <- all_datasets$data[all_datasets$dataset=="new-thyroid1"][[1]]
trainData %<>% mutate_if(is.character, as.factor) %>%
clean_names() %>%
mutate(class = relevel(class, ref = 'positive'))
recSteps = recipe(class ~ ., data = trainData) %>%
step_nzv( all_predictors() ) %>%
step_BoxCox(all_numeric()) %>%
step_normalize( all_numeric() ) %>%
step_dummy(all_nominal(), -all_outcomes() ) # create dummy variables for categorical predictors
outputFun = function(data, lev=levels(data$class), model = NULL) {
defaultS = defaultSummary(data, lev, model) # returns Accuracy and Kappa
twoClassS = twoClassSummary(data, lev, model) # returns AUC, sensitivity and specificity
prS = prSummary(data, lev, model)
out = c(defaultS, twoClassS, prS)
out['F'] = ifelse(!is.finite(out['F']), NA, out['F'])
out['AUC'] = ifelse(!is.finite(out['AUC']), NA, out['AUC'])
return(out)
}
set.seed(123)
seeds <- vector(mode = "list", length = 51)
for(i in 1:50) seeds[[i]] <- sample.int(1000, 22)
## For the last model:
seeds[[51]] <- sample.int(1000, 1)
fitControl = trainControl(
method = "cv", # k-fold cross validation
number = 10, # Number of Folds
search = "grid", # Default grid search for parameter tuning
sampling = "up", # If none, please set to NULL
summaryFunction = outputFun, # see custom function in this file
classProbs = T, # should class probabilities be returned
selectionFunction = "best", # best fold
savePredictions = 'final',
index = createResample(trainData$class, 10), seeds = seeds)
im_fit <- caret::train(class~.,, data = trainData, method = "glm", family= "binomial", metric = "AUC",
trControl = fitControl)
im_fit$results
im_fit <- caret::train(class~.,, data = trainData, method = "glm", family= "binomial", metric = "ROC",
trControl = fitControl)
im_fit$results
im_fit <- caret::train(class~.,, data = trainData, method = "glm", family= "binomial", metric = "Accuracy",
trControl = fitControl)
im_fit$results
im_fit <- caret::train(class~.,, data = trainData, method = "glm", family= "binomial",
trControl = fitControl)
im_fit$results
## simulate data
im <- twoClassSim(2000, intercept = -25, linearVars = 20)
table(im$Class)
## run caret
fitControl <- trainControl(method = "cv",summaryFunction=prSummary,
classProbs=T,savePredictions = T,verboseIter = F)
im_fit <- train(Class ~ ., data = im,method = "ranger",metric = "AUC",
trControl = fitControl)
evalm(list(im_fit))
im_fit$results
im_fit$bestTune
im_fit <- train(Class ~ ., data = im,method = "ranger",metric = "ROC",
trControl = fitControl)
## run caret
fitControl <- trainControl(method = "cv",summaryFunction=twoClassSummary,
classProbs=T,savePredictions = T,verboseIter = F)
im_fit <- train(Class ~ ., data = im,method = "ranger",metric = "ROC",
trControl = fitControl)
## run caret
fitControl <- trainControl(method = "cv",summaryFunction=twoClassSummary,
classProbs=T,savePredictions = T,verboseIter = F,
seeds=2021)
im_fit <- train(Class ~ ., data = im,method = "ranger",metric = "ROC",
trControl = fitControl)
sample.int(1000, 22)
set.seed(123)
seeds <- vector(mode = "list", length = 11)
for(i in 1:10) seeds[[i]] <- sample.int(2000, 6)
## For the last model:
seeds[[11]] <- sample.int(2000, 1)
## run caret
fitControl <- trainControl(method = "cv",summaryFunction=twoClassSummary,
classProbs=T,savePredictions = T,verboseIter = F,
seeds=2021)
## run caret
fitControl <- trainControl(method = "cv",summaryFunction=twoClassSummary,
classProbs=T,savePredictions = T,verboseIter = F,
seeds=seeds)
im_fit <- train(Class ~ ., data = im,method = "ranger",metric = "ROC",
trControl = fitControl)
im_fit$results
## run caret
fitControl <- trainControl(method = "cv",summaryFunction=prSummary,
classProbs=T,savePredictions = T,verboseIter = F,
seeds=seeds)
im_fit <- train(Class ~ ., data = im,method = "ranger",metric = "AUC",
trControl = fitControl)
im_fit$results
outputFun = function(data, lev=levels(data$class), model = NULL) {
defaultS = defaultSummary(data, lev, model) # returns Accuracy and Kappa
twoClassS = twoClassSummary(data, lev, model) # returns AUC, sensitivity and specificity
prS = prSummary(data, lev, model)
out = c(defaultS, twoClassS, prS)
out['F'] = ifelse(!is.finite(out['F']), NA, out['F'])
out['AUC'] = ifelse(!is.finite(out['AUC']), NA, out['AUC'])
return(out)
}
## run caret
fitControl <- trainControl(method = "cv",summaryFunction=outputFun,
classProbs=T,savePredictions = T,verboseIter = F,
seeds=seeds)
im_fit <- train(Class ~ ., data = im, method = "ranger",metric = "AUC",
trControl = fitControl)
im_fit$results
im_fit <- train(Class ~ ., data = im, method = "ranger",metric = "ROC",
trControl = fitControl)
im_fit$results
